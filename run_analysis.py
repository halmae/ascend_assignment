"""
Effective Orderbook Analysis ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (v2 - Memory Efficient)

ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ìµœì í™” ë²„ì „

ì‚¬ìš©ë²•:
    # ë‹¨ì¼ ì²˜ë¦¬ (ê¶Œì¥)
    python run_analysis.py --single ./data/research --name Research
    python run_analysis.py --single ./data/validation --name Validation
    
    # ìˆœì°¨ ë¹„êµ (ë©”ëª¨ë¦¬ ì ˆì•½)
    python run_analysis.py --research ./data/research --validation ./data/validation --sequential
    
    # ë³‘ë ¬ ë¹„êµ (ë©”ëª¨ë¦¬ ë§ì„ ë•Œë§Œ)
    python run_analysis.py --research ./data/research --validation ./data/validation
"""
import sys
import gc
import argparse
import json
from pathlib import Path
from typing import Optional, Dict

sys.path.insert(0, str(Path(__file__).parent))

from src.stream_processor import EffectiveOrderbookProcessor
from src.memory_efficient_streamer import MemoryEfficientStreamer, ChunkConfig, create_streamer
from src.results import ProcessingResult, compare_results


def process_dataset_memory_efficient(data_dir: str, 
                                      dataset_name: str,
                                      output_dir: Optional[str] = None) -> Dict:
    """
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë‹¨ì¼ ë°ì´í„°ì…‹ ì²˜ë¦¬
    
    ì²˜ë¦¬ í›„ ê²°ê³¼ë§Œ ë°˜í™˜í•˜ê³  ëª¨ë“  ë¦¬ì†ŒìŠ¤ í•´ì œ
    """
    print(f"\n{'='*70}")
    print(f"ğŸš€ Processing: {dataset_name}")
    print(f"ğŸ“‚ Directory: {data_dir}")
    print(f"{'='*70}")
    
    # 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìŠ¤íŠ¸ë¦¬ë¨¸ ìƒì„±
    chunk_config = ChunkConfig(
        orderbook_chunk_size=2_000_000,   # 200ë§Œ rowsì”©
        trades_chunk_size=500_000,         # 50ë§Œ rowsì”©
        ticker_chunk_size=20_000,          # 2ë§Œ rowsì”©
        liquidation_chunk_size=5_000,      # 5ì²œ rowsì”©
    )
    
    streamer = create_streamer(data_dir, chunk_config=chunk_config)
    
    # 2. Processor ìƒì„±
    processor = EffectiveOrderbookProcessor(
        dataset_name=dataset_name,
        stale_threshold_ms=50.0,
        liquidation_cooldown_ms=5000.0
    )
    
    # 3. ì´ë²¤íŠ¸ ì²˜ë¦¬
    event_count = 0
    last_progress_report = 0
    report_interval = 500_000  # 50ë§Œ ì´ë²¤íŠ¸ë§ˆë‹¤ ë³´ê³ 
    
    try:
        while streamer.has_more_events():
            event = streamer.get_next_event()
            if event:
                processor.add_event(event)
                event_count += 1
                
                # ì§„í–‰ ìƒí™© ë³´ê³ 
                if event_count - last_progress_report >= report_interval:
                    progress = streamer.get_progress()
                    ob_size = 0
                    if processor.current_orderbook:
                        ob_size = len(processor.current_orderbook.bid_levels) + len(processor.current_orderbook.ask_levels)
                    
                    print(f"  ğŸ“Š Processed {event_count:,} events | OB: {progress.get('orderbook_events', 0):,} | "
                          f"TR: {progress.get('trade_events', 0):,} | TK: {progress.get('ticker_events', 0):,} | "
                          f"OB size: {ob_size:,}")
                    last_progress_report = event_count
        
        # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        processor.process_buffer()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì²˜ë¦¬ ì¤‘ë‹¨ë¨")
    
    # 4. ê²°ê³¼ ì¶”ì¶œ (ë©”ëª¨ë¦¬ í•´ì œ ì „)
    result = processor.get_result()
    result.print_summary()
    
    # 5. ê²°ê³¼ ì €ì¥
    if output_dir:
        out_path = Path(output_dir)
        out_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ì¤‘...")
        result.save_outputs(str(out_path))
        result.to_json(str(out_path / f"{dataset_name.lower()}_full.json"))
    
    # ê²°ê³¼ ìš”ì•½ë§Œ ì¶”ì¶œ (ë©”ëª¨ë¦¬ ì ˆì•½)
    summary = {
        'dataset_name': result.dataset_name,
        'processing_time_sec': result.processing_time_sec,
        'stats': result.stats,
        'decision_counts': result.decision_counts,
        'tradability_counts': result.tradability_counts,
        'trade_validity_rate': result.trade_validity_rate,
        'allowed_rate': result.allowed_rate,
        'halted_rate': result.halted_rate,
        'state_transitions_count': len(result.state_transitions),
        'decisions_count': len(result.decisions_log),
    }
    
    # 6. ë©”ëª¨ë¦¬ í•´ì œ
    print("\nğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
    streamer.close()
    del streamer
    del processor
    del result
    gc.collect()
    
    print(f"âœ… {dataset_name} ì²˜ë¦¬ ì™„ë£Œ\n")
    
    return summary


def run_sequential_comparison(research_dir: str, 
                               validation_dir: str,
                               output_base: str = "./output"):
    """
    ìˆœì°¨ì  ë¹„êµ (ë©”ëª¨ë¦¬ ì ˆì•½)
    
    Research ì²˜ë¦¬ â†’ ê²°ê³¼ ì €ì¥ â†’ ë©”ëª¨ë¦¬ í•´ì œ â†’ Validation ì²˜ë¦¬ â†’ ë¹„êµ
    """
    print("\n" + "="*70)
    print("ğŸ“Š Sequential Comparison Mode (ë©”ëª¨ë¦¬ ì ˆì•½)")
    print("="*70)
    
    # Research ì²˜ë¦¬
    research_output = f"{output_base}/historical"
    research_summary = process_dataset_memory_efficient(
        research_dir, "Research", research_output
    )
    
    # Validation ì²˜ë¦¬  
    validation_output = f"{output_base}/validation"
    validation_summary = process_dataset_memory_efficient(
        validation_dir, "Validation", validation_output
    )
    
    # ë¹„êµ ì¶œë ¥
    print_comparison(research_summary, validation_summary)
    
    # ë¹„êµ ê²°ê³¼ ì €ì¥
    comparison = {
        'research': research_summary,
        'validation': validation_summary,
        'comparison': calculate_comparison(research_summary, validation_summary)
    }
    
    comparison_file = f"{output_base}/comparison.json"
    with open(comparison_file, 'w') as f:
        json.dump(comparison, f, indent=2)
    print(f"\nğŸ“ ë¹„êµ ê²°ê³¼ ì €ì¥: {comparison_file}")


def print_comparison(research: Dict, validation: Dict):
    """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
    print(f"\n{'='*75}")
    print(f"ğŸ“Š Research vs Validation ë¹„êµ")
    print(f"{'='*75}")
    
    print(f"\n{'ì§€í‘œ':<40} {'Research':>12} {'Validation':>12} {'ì°¨ì´':>10}")
    print(f"{'-'*75}")
    
    # Trade Validity
    r_tv = research.get('trade_validity_rate', 0)
    v_tv = validation.get('trade_validity_rate', 0)
    print(f"{'Trade Validity Rate':<40} {r_tv:>11.1%} {v_tv:>11.1%} {v_tv - r_tv:>+9.1%}")
    
    print(f"{'-'*75}")
    
    # Decision Distribution
    r_allowed = research.get('allowed_rate', 0)
    v_allowed = validation.get('allowed_rate', 0)
    r_halted = research.get('halted_rate', 0)
    v_halted = validation.get('halted_rate', 0)
    
    print(f"{'ALLOWED %':<40} {r_allowed:>11.1%} {v_allowed:>11.1%} {v_allowed - r_allowed:>+9.1%}")
    print(f"{'HALTED %':<40} {r_halted:>11.1%} {v_halted:>11.1%} {v_halted - r_halted:>+9.1%}")
    
    print(f"{'-'*75}")
    
    # State Transitions
    r_trans = research.get('state_transitions_count', 0)
    v_trans = validation.get('state_transitions_count', 0)
    print(f"{'State Transitions':<40} {r_trans:>11,} {v_trans:>11,}")
    
    print(f"\n{'='*75}")
    
    # ì¸ì‚¬ì´íŠ¸
    print(f"\n[í•µì‹¬ ì¸ì‚¬ì´íŠ¸]")
    
    if v_tv - r_tv < -0.05:
        print(f"  âš ï¸ Validationì—ì„œ Trade Validityê°€ {-(v_tv - r_tv):.1%}p ë‚®ìŒ â†’ Dirty Data ì˜í–¥")
    
    if v_allowed - r_allowed < -0.05:
        print(f"  âš ï¸ Validationì—ì„œ ALLOWEDê°€ {-(v_allowed - r_allowed):.1%}p ë‚®ìŒ â†’ Uncertainty ì¦ê°€")
    
    if v_halted - r_halted > 0.05:
        print(f"  âš ï¸ Validationì—ì„œ HALTEDê°€ {v_halted - r_halted:.1%}p ë†’ìŒ â†’ íŒë‹¨ ì¤‘ë‹¨ êµ¬ê°„ ì¦ê°€")
    
    if v_allowed >= 0.8:
        print(f"  âœ… Validationì—ì„œë„ 80% ì´ìƒ ALLOWED ìœ ì§€")


def calculate_comparison(research: Dict, validation: Dict) -> Dict:
    """ë¹„êµ ì§€í‘œ ê³„ì‚°"""
    return {
        'trade_validity_diff': validation.get('trade_validity_rate', 0) - research.get('trade_validity_rate', 0),
        'allowed_diff': validation.get('allowed_rate', 0) - research.get('allowed_rate', 0),
        'halted_diff': validation.get('halted_rate', 0) - research.get('halted_rate', 0),
        'transitions_diff': validation.get('state_transitions_count', 0) - research.get('state_transitions_count', 0),
    }


def run_single(data_dir: str, dataset_name: str = "Dataset", output_dir: str = "./output"):
    """ë‹¨ì¼ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
    out_path = f"{output_dir}/{dataset_name.lower()}"
    process_dataset_memory_efficient(data_dir, dataset_name, out_path)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Effective Orderbook Analysis (Memory Efficient)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ë‹¨ì¼ ì²˜ë¦¬
  python run_analysis.py --single ./data/research --name Research
  python run_analysis.py --single ./data/validation --name Validation
  
  # ìˆœì°¨ ë¹„êµ (ê¶Œì¥ - ë©”ëª¨ë¦¬ ì ˆì•½)
  python run_analysis.py --research ./data/research --validation ./data/validation
  
  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì§€ì •
  python run_analysis.py --single ./data/research --name Research --output ./results
        """
    )
    parser.add_argument("--research", type=str, help="Research ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--validation", type=str, help="Validation ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--single", type=str, help="ë‹¨ì¼ ë°ì´í„°ì…‹ ì²˜ë¦¬")
    parser.add_argument("--name", type=str, default="Dataset", help="ë°ì´í„°ì…‹ ì´ë¦„")
    parser.add_argument("--output", type=str, default="./output", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    if args.single:
        run_single(args.single, args.name, args.output)
    elif args.research and args.validation:
        run_sequential_comparison(args.research, args.validation, args.output)
    else:
        parser.print_help()